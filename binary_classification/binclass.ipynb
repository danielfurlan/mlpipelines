{"cells":[{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"import lightgbm as lgb\nfrom sklearn import metrics\nfrom sklearn.preprocessing import StandardScaler, RobustScaler, Normalizer\nfrom sklearn.metrics import confusion_matrix,accuracy_score, roc_curve, auc\nimport seaborn as sns\nimport matplotlib.pyplot as plt \n\nclass GBM:\n    \n\n    def __init__(self,train,valid,test,target):\n        self.train = train\n        self.valid = valid\n        self.test = test\n        self.target = target\n        self.feature_cols = self.train.columns.drop(self.target)\n  \n    def train_GBM(self,over,n):\n\n\n        dtrain = lgb.Dataset(self.train[self.feature_cols], label=self.train[self.target])\n        dvalid = lgb.Dataset(self.valid[self.feature_cols], label=self.valid[self.target])\n    \n        if over:\n            param = {'num_leaves': 31, 'objective': 'binary', \"max_depth\": 3,\n             'metric': 'auc', 'seed': 7, 'reg_alpha':0.8, 'reg_lambda':0.8}\n            print(f\"Regularization : l1 = {param['reg_alpha']}, l2 = {param['reg_lambda']}\")\n        else:\n            param = {'num_leaves': 64, 'objective': 'binary',\n             'metric': 'auc', 'seed': 7}\n            print(f\"No regularization!\")\n        \n        evals_result = {} \n        bst = lgb.train(param, dtrain, num_boost_round=1000, valid_sets=[dvalid,dtrain], \n                    early_stopping_rounds=10, verbose_eval=10, evals_result=evals_result)\n        nameModel = \"Model \" + str(n) +\".txt\"\n        import joblib\n        # save model\n        joblib.dump(bst, nameModel)\n    \n        self.evaluate_GBM(bst,evals_result,n)  \n        \n    def evaluate_GBM(self,bst,evals_result,n):\n    \n        valid_pred = bst.predict(self.valid[self.feature_cols])\n        valid_score = metrics.roc_auc_score(self.valid[self.target], valid_pred)\n    \n        test_pred = bst.predict(self.test[self.feature_cols])\n        test_score = metrics.roc_auc_score(self.test[self.target], test_pred)\n    \n        print(f\"Validation AUC score: {valid_score:.4f}\")\n        print(f\"Test AUC score: {test_score:.4f}\")\n        if valid_score > 0.95:\n            print(\"\\n\\nYou're overfitting. Better rerun with parameter 'over' as 'True'. If already set as so\\\n            it is time to fine tune your model parameters!\\n\\n\")\n    \n        self.plot(evals_result,valid_pred,bst,n)\n    \n    def plot(self,evals_result,valid_pred,bst,n):\n        \n        global conf\n        global auc\n        global roc\n        global y_predictions\n        #global n\n            \n        if evals_result != None:\n            acu[n] = evals_result\n            fig1 = plt.figure(figsize=(45,10))\n        #print('Plot metrics during training... Our metric : ', param[\"metric\"])\n        #print(\"evals_ results : \", evals_result)\n            lgb.plot_metric(evals_result, metric='auc',figsize=(35,10))\n            plt.xlabel('Iterations',fontsize=20)\n            plt.ylabel('auc',fontsize=20)\n            plt.xticks(fontsize=20)\n            plt.yticks(fontsize=20)\n            plt.title(\"AUC during training\",fontsize=20)\n            plt.legend(fontsize=20)\n            plt.show()\n\n\n            ##### CONFUSION MATRIX\n        th = 0.5\n        y_pred_class = valid_pred > th\n        y_predictions[n] = y_pred_class\n        cm = confusion_matrix(self.valid[self.target], y_pred_class)\n        tn, fp, fn, tp = cm.ravel()\n        fpr = fp / (fp + tn)\n        fnr = fn / (tp + fn)\n        tnr = tn / (tn + fp)\n        tpr = tp / (tp + fn)\n        numberModel = n\n        conf[n] = {'fpr':f'{fpr:.3f}','fnr': f'{fnr:.3f}', 'tnr' : f'{tnr:.3f}', \"tpr\": f'{tpr:.3f}'}\n        if n > 1 and fpr != 0 and fnr != 0 and tnr != 0 and tpr != 0:\n            conf[\"ratio \" + str(n) + \"/\" + str(n-1)] = {\"fp\":f'{float(conf[n][\"fpr\"])/float(conf[n-1][\"fpr\"]):.3f}', \\\n                                                        \"fn\":f'{float(conf[n][\"fnr\"])/float(conf[n-1][\"fnr\"]):.3f}', \\\n                                                        \"tn\":f'{float(conf[n][\"tnr\"])/float(conf[n-1][\"tnr\"]):.3f}', \\\n                                                        \"tp\":f'{float(conf[n][\"tpr\"])/float(conf[n-1][\"tpr\"]):.3f}'}\n        \n        fig2 = plt.figure(figsize=(35,10))\n        fig2.add_subplot(1,2,1)\n        sns.heatmap(cm, annot = True, fmt='d', cmap=\"Blues\", vmin = 0.2,linewidths=.5,annot_kws={\"fontsize\": 20}); #cbar_kws={\"fontsize\": 20},annot_kws={\"fontsize\": 20}\n        sns.set(font_scale=2)\n        plt.title('Confusion Matrix',fontsize=20)\n        plt.ylabel('True Class',fontsize=20)\n        plt.xlabel('Predicted Class',fontsize=20)\n        plt.xticks(fontsize=20)\n        plt.yticks(fontsize=20)\n        plt.text(0.1, 0.3, f' FPR: {fpr:.3f}\\n FNR: {fnr:.3f}\\n TNR: {tnr:.3f}\\n TPR: {tpr:.3f}', style='italic',\n        bbox={'facecolor': 'white', 'alpha': 0.7, 'pad': 5}, fontsize=14)\n        \n        \n        #Print Area Under Curve\n        fig2.add_subplot(1,2,2)\n\n        false_positive_rate, recall, thresholds = roc_curve(self.valid[self.target], valid_pred)\n        roc_auc = auc(false_positive_rate, recall)\n        roc[n] = {'fpr':false_positive_rate,'recall':recall}\n        \n        plt.title('Receiver Operating Characteristic (ROC)')\n        for a in range(1,n+1):\n            plt.plot(roc[a]['fpr'], roc[a]['recall'], 'b', label = f'Model {n}')\n            \n#             plt.plot(false_positive_rate, recall, 'b', label = 'AUC = %0.3f' %roc_auc)\n            plt.legend(loc='lower right')\n            plt.plot([0,1], [0,1], 'r--')\n            plt.xlim([0.0,1.0])\n            plt.ylim([0.0,1.0])\n            plt.ylabel('Recall',fontsize=20)\n            plt.xlabel('Fall-out (1-Specificity)',fontsize=20)\n            plt.xticks(fontsize=20)\n            plt.yticks(fontsize=20)\n        \n        plt.show()    \n        display(conf) \n            \nimport pandas\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom tensorflow import keras\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\n\nclass DNN:\n    \n    def __init__(self,train,valid,test,target,inp_dim):\n        self.train = train\n        self.valid = valid\n        self.test = test\n        self.target = target\n        self.inp_dim = inp_dim\n        \n    def train_DNN(self,n):\n        #global n\n    \n        print(\"\\n\\n You are now running a DNN algorithm!\\n\\n\")\n        #train,valid, test = get_data_splits(dt)\n\n        X = self.train.drop(self.target,axis=1) #[:,0:60].astype(float)\n        Y = self.train[self.target]\n    # define model\n        model = Sequential()\n\n        model.add(Dense(self.inp_dim, activation='relu'))\n        model.add(Dense(60,activation='relu'))#input_dim=60\n        model.add(Dense(1, activation='sigmoid'))    \n        \n        # compile model\n        model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n        # Fit the model\n        history = model.fit(X, Y, epochs=10, batch_size=32, verbose=1)\n        \n        # evaluate the model\n        scores = model.evaluate(X, Y, verbose=1)\n        print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n        # save model and architecture to single file\n        model.save(\"model.h5\")\n        print(\"Saved model to disk\")\n            \n        #### Get validation scores\n        X_valid = self.valid.drop(self.target,axis=1)\n\n        from sklearn.preprocessing import MinMaxScaler\n\n        y_pred = model.predict_proba(X_valid)\n\n    #     print(\"lenght ynew : \", len(y_pred))\n    # print(\"X=%s, Predicted=%s\" % (Xnew.iloc[0], ynew[0]))\n\n    #### Plot Accuracy graph:\n        #print(history.history.keys())\n        fig = plt.figure(figsize=(35,10))\n    # history.history['accuracy']\n        plt.plot(history.history['accuracy'], color='blue', label='train')\n\n        plt.xlabel('Epochs',fontsize=20)\n        plt.ylabel('auc',fontsize=20)\n        plt.xticks(fontsize=20)\n        plt.yticks(fontsize=20)\n        plt.title(\"AUC during training\",fontsize=20)\n        plt.legend(fontsize=20)\n\n        plt.show()\n\n\nimport lightgbm as lgb\nfrom sklearn import metrics\nfrom sklearn.preprocessing import StandardScaler, RobustScaler, Normalizer\nfrom sklearn.metrics import confusion_matrix,accuracy_score, roc_curve, auc\nimport seaborn as sns\nimport matplotlib.pyplot as plt \n\n#import binGBM as gbm\n#import binDNN as dnn\nconf = {}\nacu = {}\nroc = {}\ny_predictions = {}\n\n\nclass BinaryClass:\n    \n    def __init__(self,dataframe,target=None,feature_cols=None):\n        \n        self.dataframe = dataframe\n#         self.conf = {}\n#         self.acu = {}\n#         self.roc = {}\n#         self.y_predictions = {}\n        \n        if target == None:\n            print(\"What is your target columns?\")\n            display(self.dataframe.columns)\n            \n            ans = str(input())\n            while ans not in self.dataframe.columns:\n                print(\"Please, enter a valid column name! Pay attention with spaces!\")\n                ans = str(input())\n            self.target = ans\n            self.feature_cols = dataframe.columns.drop(self.target)\n        else:\n            self.target = target\n            self.feature_cols = dataframe.columns.drop(self.target)\n        \n        self.train, self.valid, self.test = self.get_data_splits(self.dataframe)\n        \n    def get_data_splits(self,dataframe, valid_fraction=0.1):\n        \n        valid_fraction = 0.1\n        valid_size = int(len(dataframe) * valid_fraction)\n\n        train = dataframe[:-valid_size * 2]\n        # valid size == test size, last two sections of the data\n        valid = dataframe[-valid_size * 2:-valid_size]\n        test = dataframe[-valid_size:]\n        print(f\"Train size : {len(train)}\\nValidation size : {len(valid)}\\nTest size : {len(test)}\")\n        return train, valid, test\n    \n    def train_models(self,over,n):\n        gbm = GBM(self.train,self.valid,self.test,self.target)\n#         dnn = DNN()\n        \n        gbm.train_GBM(over,n)\n        print(\"Lenght of self.featcols = \", len(self.feature_cols))\n        print(\"Do you want to run a DNN algorithm? (y/n)\" )\n        answer = input()\n        if answer == 'y':\n            dnn = DNN(self.train,self.valid,self.test,self.target,len(self.feature_cols))\n            dnn.train_DNN(n)\n        \n        ","execution_count":5,"outputs":[]}],"metadata":{"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"}},"nbformat":4,"nbformat_minor":4}